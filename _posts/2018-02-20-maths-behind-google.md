---
title: "Learn how Google works by building Google"
categories:
  - University
---

Have you ever wanted to know how Google *really* works? Well, this article attempts to not only explain how Google works but to make our own search engine!

I am in no way an expert at search engines, I am an undergraduate Computer Science student at the University of Liverpool so this article is heavily researched and referenced and was created to increase my understanding of search engines.

When Google first started out in the 90s the one thing that set them apart from their competitors is that its search results always seem to show the most relevant and helpful results. Google's *page ranking* algorithm was at least 10 times better than any other search engine at the time.

Search engines typically have 3 main objectives:
1. Crawl the web and find all the web pages with public access
2. Index the data from step 1 so the pages can be searched efficiently
3. Rate the importance of each page in the database.

We'll start with crawling the web.

A webcrawler, also called a spider crawls the web looking for new websites. Once a website has been found, the spider regularly checks to see if the information has been updated and it crawls all links on that website to find other websites. 

[Wikipedia](https://en.wikipedia.org/wiki/Web_crawler) gives a good oversight into the details of a web crawler:

>A Web crawler starts with a list of URLs to visit, called the seeds. As the crawler visits these URLs, it identifies all the hyperlinks in the page and adds them to the list of URLs to visit, called the crawl frontier. URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as they were on the live web, but are preserved as ‘snapshots'.[4]

>The archive is known as the repository and is designed to store and manage the collection of web pages. The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler.[5]

>The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted.

>The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content. Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content.

>As Edwards et al. noted, "Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained."[6] A crawler must carefully choose at each step which pages to visit next.

Note that a web crawler must start with a list of URLs. You may be wondering "how did web crawlers even start out if search engines did not exist? How would one know what URLs to start with?". Well, Tim Berners-Lee kept a list in his backpocket of all the URLs of all the websites on the web at the time according to [this](https://tim.blog/2018/02/14/bob-metcalfe/). This was probably very common, since back then only 2 maybe 3 websites existed at any given time and the adoption of the world wide web was _very_ slow in the first year or two.

Tim Berners-Lee actually has a FAQ on this [website](https://www.w3.org/People/Berners-Lee/Kids.html) which answers some very interesting questions about the world wide web.

One question that you may still have is "what is the difference between the internet and the web?". The Computer History Musuem has an easy to understand [answer](http://www.computerhistory.org/revolution/networking/19/314)

>The Internet, linking your computer to other computers around the  world, is a way of transporting content. The Web is software that lets  you use that content…or contribute your own. The Web, running on the  mostly invisible Internet, is what you see and click on in your  computer’s browser.

>The Internet’s roots are in the U.S. during the late 1960s. The Web  was invented 20 years later by an Englishman working in  Switzerland—though it had many predecessors.

>To keep things “interesting,” many people use the term Internet to refer to both.

We'll start here by building a spider. We'll use Python 3x and the Python package [Scrapy](https://scrapy.org/) to scrape websites. In this day and age it is useless to try and make our own spider, since packages exist which are alot better than anything we could make. I'll describe spiders as much as possible so we don't skip over how they work, just how to actually make one.

Firstly, we'll use a virtual environment in Python so installing scripts, modules and other things won't mess up our system-wide python install. Check out [this](https://virtualenv.pypa.io/en/stable/userguide/) user guide for how to use VirtualENV.

This is really useful because often we want to install specific versions of packages that conflict with our system wide versions.

You can download Python 3x [here](https://www.python.org/).

Click [here](https://docs.scrapy.org/en/latest/intro/install.html#intro-install) for an installation guide for Scrapy.

In short, all you need to do (assuming you have a Unix / Linux / Mac OS terminal):

```
sudo apt update
pip install virtualenv
virtualenv Google
source google/bin/activate
pip install Scrapy
```

Okay, so we have Scrapy installed. We can now build our web crawler!

We need to design what the crawler will do, because making software right off the bat is just plain stupid. So we want a box called website and items in the website with links in them, where each link is a mini-box with other links in it until it stops being a box. This is kind of confusing so...

Each website will contain links in it that send it to other websites. Each of those websites also have links in them which send them off to other websites. So we start off with one URL (probably Hacker News) and each link will also be a website with lniks and so on. 

```
+-----------------------------------------------------------------+
|                    HACKERNEWS                                   |
|                                                                 |
|                                                                 |
|    +----------------------+   +---------------------------+     |
|    |      BBC             |   |          NETFLIX          |     |
|    |                      |   |                           |     |
|    |    +---------------+ |   |   +------+                |     |
|    |    |    GUARDIAN   | |   |   |  BBC |       +------+ |     |
|    |    |               | |   |   |      |       |  Yahoo |     |
|    |    |               | |   |   +------+       |      | |     |
|    |    |               | |   |                  +------+ |     |
|    |    |               | |   |                           |     |
|    |    +---------------+ |   |                           |     |
|    |                      |   |                           |     |
|    +----------------------+   +---------------------------+     |
|                                                                 |
|                                                                 |
|                                                                 |
|                                                                 |
|                                                                 |
+-----------------------------------------------------------------+
```

Sorry if you're on mobile.

So it's links all the way down. Something cool to note here is that links inside links inside links can point to the original parent link.

So what we want is for our crawler to be able to tell the parents and children links apart. This is really important in future pageranking algorithms, but at the moment we just need to know this.

One way we could represent this is with a directed graph like so:

```
                                         +--------------+
                                         |    YOUTUBE   |
                                   +----->              |
                                   |     +--------------+
                         +---------+
+---------+------------->+  NETFLIX+-------^---------------+
| BBC     |              +---------+       |   GOOGLE      |
+---------+---+                    |       |               |
              |                    |       +---------------+
              |                    |
              |                    |         +----------------+
              |                    +--------->   INSTAGRAM    |
            +-v----------+                   +----------------+
            |  UNIVERSITY|
            |            |
            +------------+

```

Something important to note is that it's entirely possible for a child to be a parent of it's parent, like so:

```
+--------------+----> +-------------+
|    NETFLIX   |      |    BBC      |
|              |      |             |
|              |      |             |
+--------------+ <----+-------------+
```

We'll start a new Scrapy project by using the command:

```
scrapy startproject Google
```

Something to note here is that many features of this part of the tutorial are unsurprinsgly taken from the [Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html#intro-tutorialhttps://docs.scrapy.org/en/latest/intro/tutorial.html#intro-tutorial).

This will create a folder directory that looks like this:

```
Google/
    scrapy.cfg            # deploy configuration file

    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py
```

Also something to note here is that Google is a registered trademark and I'm only using their name because they're an amazing search engine please don't sue me Google.


Here is an image of a very small web

![img](https://screenshotscdn.firefoxusercontent.com/images/8507a511-57fd-42f7-b2ee-8910f830f4f0.png)
Taken from [here](https://www.rose-hulman.edu/~bryan/googleFinalVersionFixed.pdf).

We will have an importance score that will represent how important a webpage is. One of the core ideas of assigning a score is that the page's score is derived from the links made to the page from other web pages. The links to a given page are called the _backlinks_ for that page.

In such a scoring system a democracy is created where web pages "vote" for the importance of web pages by lniking to them.

The web seen in the imaeg is an example of a _directed graph_, a graph that holds direction. A typical graph will consist of a set of vertices (in this context, they are called the web pages) and a set of edges. Each edge joins a pair of vertices (webpages). The graph is considered directed because each edge has a direction.

An approach one could make is to take $$x_k$$ as the number of backlinks for any page, k. In  the image above we then have:

$$x_1 - 2, x_2 = 1, x_3 = 3, x_4 = 2 $$

As can be seen here the most "important" web page is $$x_3$$ since it is linked to by 3 pages.

This approach ignores a large problem, mainly that a link from an already important page should have more weight to it than a link from a non-important page. In the image above pages 1 and 4 both have two backlinks: each links to the other, but page 1's backlink is from an important web page, page 3. While page 4's backlink is from an unimportant page 1.

In our new and improved version we don't want pages to be able to "vote" for themselves. We don't want a page to gain popularity by linking to lots of other pages. If page j contains $$x_j$$ links, one of them linking to page k, then we will boost page k's score by $$\frac{x_j}{n_j}$$.

In this version each webpage gets a total of one vote, weighted by that web page's score that is evenly divided up among all of its outgoing links.

# References
[0] - https://nlp.stanford.edu/IR-book/html/htmledition/web-crawling-and-indexes-1.html
